{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (HashingTF and IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20,[5,6,9],[0.0,0.6931471805599453,1.3862943611198906]),0]\n",
      "[(20,[3,5,12,14,18],[1.3862943611198906,0.0,0.28768207245178085,0.28768207245178085,0.28768207245178085]),0]\n",
      "[(20,[5,12,14,18],[0.0,0.5753641449035617,0.28768207245178085,0.28768207245178085]),1]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
    "\n",
    "val sentenceData = sqlContext.createDataFrame(Seq(\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (0, \"I wish Java could use case classes\"),\n",
    "  (1, \"Logistic regression models are neat\")\n",
    ")).toDF(\"label\", \"sentence\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(sentenceData)\n",
    "val hashingTF = new HashingTF().setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\n",
    "val featurizedData = hashingTF.transform(wordsData)\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "val rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"features\", \"label\").take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.018490654602646827,-0.016248732805252075,0.04528368394821883]]\n",
      "[[0.05958533100783825,0.023424440695505054,-0.027310076036623544]]\n",
      "[[-0.011055880039930344,0.020988055132329465,0.042608972638845444]]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "\n",
    "// Input data: Each row is a bag of words from a sentence or document.\n",
    "val documentDF = sqlContext.createDataFrame(Seq(\n",
    "  \"Hi I heard about Spark\".split(\" \"),\n",
    "  \"I wish Java could use case classes\".split(\" \"),\n",
    "  \"Logistic regression models are neat\".split(\" \")\n",
    ").map(Tuple1.apply)).toDF(\"text\")\n",
    "\n",
    "// Learn a mapping from words to Vectors.\n",
    "val word2Vec = new Word2Vec().setInputCol(\"text\").setOutputCol(\"result\").setVectorSize(3).setMinCount(0)\n",
    "val model = word2Vec.fit(documentDF)\n",
    "val result = model.transform(documentDF)\n",
    "result.select(\"result\").take(3).foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedArray(hi, i, heard, about, spark),0]\n",
      "[WrappedArray(i, wish, java, could, use, case, classes),1]\n",
      "[WrappedArray(logistic,regression,models,are,neat),2]\n",
      "[WrappedArray(hi, i, heard, about, spark),0]\n",
      "[WrappedArray(i, wish, java, could, use, case, classes),1]\n",
      "[WrappedArray(logistic, regression, models, are, neat),2]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "\n",
    "val sentenceDataFrame = sqlContext.createDataFrame(Seq(\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (1, \"I wish Java could use case classes\"),\n",
    "  (2, \"Logistic,regression,models,are,neat\")\n",
    ")).toDF(\"label\", \"sentence\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\n",
    "val regexTokenizer = new RegexTokenizer().setInputCol(\"sentence\").setOutputCol(\"words\").setPattern(\"\\\\W\") // alternatively .setPattern(\"\\\\w+\").setGaps(false)\n",
    "\n",
    "val tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"words\", \"label\").take(3).foreach(println)\n",
    "val regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"words\", \"label\").take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                 raw|            filtered|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[I, saw, the, red...|  [saw, red, baloon]|\n",
      "|  1|[Mary, had, a, li...|[Mary, little, lamb]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val remover = new StopWordsRemover().setInputCol(\"raw\").setOutputCol(\"filtered\")\n",
    "\n",
    "val dataSet = sqlContext.createDataFrame(Seq(\n",
    "  (0, Seq(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n",
    "  (1, Seq(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n",
    ")).toDF(\"id\", \"raw\")\n",
    "\n",
    "remover.transform(dataSet).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *n*-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(Hi I, I heard, heard about, about Spark)\n",
      "List(I wish, wish Java, Java could, could use, use case, case classes)\n",
      "List(Logistic regression, regression models, models are, are neat)\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.NGram\n",
    "\n",
    "val wordDataFrame = sqlContext.createDataFrame(Seq(\n",
    "  (0, Array(\"Hi\", \"I\", \"heard\", \"about\", \"Spark\")),\n",
    "  (1, Array(\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\")),\n",
    "  (2, Array(\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"))\n",
    ")).toDF(\"label\", \"words\")\n",
    "\n",
    "val ngram = new NGram().setInputCol(\"words\").setOutputCol(\"ngrams\")\n",
    "val ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.take(3).map(_.getAs[Stream[String]](\"ngrams\").toList).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n",
      "[1.0]\n",
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Binarizer\n",
    "\n",
    "val data = Array((0, 0.1), (1, 0.8), (2, 0.2))\n",
    "val dataFrame = sqlContext.createDataFrame(data).toDF(\"label\", \"feature\")\n",
    "\n",
    "val binarizer: Binarizer = new Binarizer().setInputCol(\"feature\").setOutputCol(\"binarized_feature\").setThreshold(0.5)\n",
    "\n",
    "val binarizedDataFrame = binarizer.transform(dataFrame)\n",
    "val binarizedFeatures = binarizedDataFrame.select(\"binarized_feature\")\n",
    "binarizedFeatures.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         pcaFeatures|\n",
      "+--------------------+\n",
      "|[1.64857282308838...|\n",
      "|[-4.6451043317815...|\n",
      "|[-6.4288805356764...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PCA\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val data = Array(\n",
    "  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),\n",
    "  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    ")\n",
    "val df = sqlContext.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "val pca = new PCA().setInputCol(\"features\").setOutputCol(\"pcaFeatures\").setK(3).fit(df)\n",
    "val pcaDF = pca.transform(df)\n",
    "val result = pcaDF.select(\"pcaFeatures\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolynomialExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0,4.0,-8.0,2.3,-4.6,9.2,5.289999999999999,-10.579999999999998,12.166999999999996]]\n",
      "[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]]\n",
      "[[0.6,0.36,0.216,-1.1,-0.66,-0.396,1.2100000000000002,0.7260000000000001,-1.3310000000000004]]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PolynomialExpansion\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val data = Array(\n",
    "  Vectors.dense(-2.0, 2.3),\n",
    "  Vectors.dense(0.0, 0.0),\n",
    "  Vectors.dense(0.6, -1.1)\n",
    ")\n",
    "val df = sqlContext.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "val polynomialExpansion = new PolynomialExpansion().setInputCol(\"features\").setOutputCol(\"polyFeatures\").setDegree(3)\n",
    "val polyDF = polynomialExpansion.transform(df)\n",
    "polyDF.select(\"polyFeatures\").take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Cosine Transform (DCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         featuresDCT|\n",
      "+--------------------+\n",
      "|[1.0,-1.148050297...|\n",
      "|[-1.0,3.378492794...|\n",
      "|[4.0,9.3044534219...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.DCT\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val data = Seq(\n",
    "  Vectors.dense(0.0, 1.0, -2.0, 3.0),\n",
    "  Vectors.dense(-1.0, 2.0, 4.0, -7.0),\n",
    "  Vectors.dense(14.0, -2.0, -5.0, 1.0))\n",
    "\n",
    "val df = sqlContext.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "val dct = new DCT().setInputCol(\"features\").setOutputCol(\"featuresDCT\").setInverse(false)\n",
    "\n",
    "val dctDf = dct.transform(df)\n",
    "dctDf.select(\"featuresDCT\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val df = sqlContext.createDataFrame(\n",
    "  Seq((0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"))\n",
    ").toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"category\").setOutputCol(\"categoryIndex\")\n",
    "\n",
    "val indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|originalCategory|\n",
      "+---+----------------+\n",
      "|  0|               a|\n",
      "|  1|               b|\n",
      "|  2|               c|\n",
      "|  3|               a|\n",
      "|  4|               a|\n",
      "|  5|               c|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString}\n",
    "\n",
    "val df = sqlContext.createDataFrame(Seq(\n",
    "  (0, \"a\"),\n",
    "  (1, \"b\"),\n",
    "  (2, \"c\"),\n",
    "  (3, \"a\"),\n",
    "  (4, \"a\"),\n",
    "  (5, \"c\")\n",
    ")).toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"category\").setOutputCol(\"categoryIndex\").fit(df)\n",
    "val indexed = indexer.transform(df)\n",
    "\n",
    "val converter = new IndexToString().setInputCol(\"categoryIndex\").setOutputCol(\"originalCategory\")\n",
    "\n",
    "val converted = converter.transform(indexed)\n",
    "converted.select(\"id\", \"originalCategory\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "+---+-------------+\n",
      "| id|  categoryVec|\n",
      "+---+-------------+\n",
      "|  0|(2,[0],[1.0])|\n",
      "|  1|    (2,[],[])|\n",
      "|  2|(2,[1],[1.0])|\n",
      "|  3|(2,[0],[1.0])|\n",
      "|  4|(2,[0],[1.0])|\n",
      "|  5|(2,[1],[1.0])|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
    "\n",
    "val df = sqlContext.createDataFrame(Seq(\n",
    "  (0, \"a\"),\n",
    "  (1, \"b\"),\n",
    "  (2, \"c\"),\n",
    "  (3, \"a\"),\n",
    "  (4, \"a\"),\n",
    "  (5, \"c\")\n",
    ")).toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"category\").setOutputCol(\"categoryIndex\").fit(df)\n",
    "val indexed = indexer.transform(df)\n",
    "indexed.show()\n",
    "\n",
    "val encoder = new OneHotEncoder().setInputCol(\"categoryIndex\").setOutputCol(\"categoryVec\")\n",
    "val encoded = encoder.transform(indexed)\n",
    "encoded.select(\"id\", \"categoryVec\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose 351 categorical features: 645, 69, 365, 138, 101, 479, 333, 249, 0, 555, 666, 88, 170, 115, 276, 308, 5, 449, 120, 247, 614, 677, 202, 10, 56, 533, 142, 500, 340, 670, 174, 42, 417, 24, 37, 25, 257, 389, 52, 14, 504, 110, 587, 619, 196, 559, 638, 20, 421, 46, 93, 284, 228, 448, 57, 78, 29, 475, 164, 591, 646, 253, 106, 121, 84, 480, 147, 280, 61, 221, 396, 89, 133, 116, 1, 507, 312, 74, 307, 452, 6, 248, 60, 117, 678, 529, 85, 201, 220, 366, 534, 102, 334, 28, 38, 561, 392, 70, 424, 192, 21, 137, 165, 33, 92, 229, 252, 197, 361, 65, 97, 665, 583, 285, 224, 650, 615, 9, 53, 169, 593, 141, 610, 420, 109, 256, 225, 339, 77, 193, 669, 476, 642, 637, 590, 679, 96, 393, 647, 173, 13, 41, 503, 134, 73, 105, 2, 508, 311, 558, 674, 530, 586, 618, 166, 32, 34, 148, 45, 161, 279, 64, 689, 17, 149, 584, 562, 176, 423, 191, 22, 44, 59, 118, 281, 27, 641, 71, 391, 12, 445, 54, 313, 611, 144, 49, 335, 86, 672, 172, 113, 681, 219, 419, 81, 230, 362, 451, 76, 7, 39, 649, 98, 616, 477, 367, 535, 103, 140, 621, 91, 66, 251, 668, 198, 108, 278, 223, 394, 306, 135, 563, 226, 3, 505, 80, 167, 35, 473, 675, 589, 162, 531, 680, 255, 648, 112, 617, 194, 145, 48, 557, 690, 63, 640, 18, 282, 95, 310, 50, 67, 199, 673, 16, 585, 502, 338, 643, 31, 336, 613, 11, 72, 175, 446, 612, 143, 43, 250, 231, 450, 99, 363, 556, 87, 203, 671, 688, 104, 368, 588, 40, 304, 26, 258, 390, 55, 114, 171, 139, 418, 23, 8, 75, 119, 58, 667, 478, 536, 82, 620, 447, 36, 168, 146, 30, 51, 190, 19, 422, 564, 305, 107, 4, 136, 506, 79, 195, 474, 664, 532, 94, 283, 395, 332, 528, 644, 47, 15, 163, 200, 68, 62, 277, 691, 501, 90, 111, 254, 227, 337, 122, 83, 309, 560, 639, 676, 222, 592, 364, 100\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|             indexed|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "\n",
    "val data = sqlContext.read.format(\"libsvm\").load(\"file:////usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "val indexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexed\").setMaxCategories(10)\n",
    "\n",
    "val indexerModel = indexer.fit(data)\n",
    "\n",
    "val categoricalFeatures: Set[Int] = indexerModel.categoryMaps.keys.toSet\n",
    "println(s\"Chose ${categoricalFeatures.size} categorical features: \" +\n",
    "  categoricalFeatures.mkString(\", \"))\n",
    "\n",
    "// Create new column \"indexed\" with categorical values transformed to indices\n",
    "val indexedData = indexerModel.transform(data)\n",
    "indexedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        normFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        normFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val dataFrame = sqlContext.read.format(\"libsvm\").load(\"file:////usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "// Normalize each Vector using $L^1$ norm.\n",
    "val normalizer = new Normalizer().setInputCol(\"features\").setOutputCol(\"normFeatures\").setP(1.0)\n",
    "\n",
    "val l1NormData = normalizer.transform(dataFrame)\n",
    "l1NormData.show()\n",
    "\n",
    "// Normalize each Vector using $L^\\infty$ norm.\n",
    "val lInfNormData = normalizer.transform(dataFrame, normalizer.p -> Double.PositiveInfinity)\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val dataFrame = sqlContext.read.format(\"libsvm\").load(\"file:////usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeatures\").setWithStd(true).setWithMean(false)\n",
    "\n",
    "// Compute summary statistics by fitting the StandardScaler.\n",
    "val scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[158,159,160...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[152,153,154...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[151,152,153...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[129,130,131...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[158,159,160...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[99,100,101,...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[127,128,129...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[153,154,155...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[151,152,153...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[129,130,131...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[154,155,156...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[150,151,152...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|\n",
      "|  0.0|(692,[152,153,154...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[97,98,99,12...|[0.5,0.5,0.5,0.5,...|\n",
      "|  1.0|(692,[124,125,126...|[0.5,0.5,0.5,0.5,...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.MinMaxScaler\n",
    "\n",
    "val dataFrame = sqlContext.read.format(\"libsvm\").load(\"file:////usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "val scaler = new MinMaxScaler().setInputCol(\"features\").setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "// Compute summary statistics and generate MinMaxScalerModel\n",
    "val scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "// rescale each feature to range [min, max].\n",
    "val scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity)\n",
    "\n",
    "val data = Array(-0.5, -0.3, 0.0, 0.2)\n",
    "val dataFrame = sqlContext.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "val bucketizer = new Bucketizer().setInputCol(\"features\").setOutputCol(\"bucketedFeatures\").setSplits(splits)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(dataFrame)\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElementwiseProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------------+\n",
      "| id|       vector|transformedVector|\n",
      "+---+-------------+-----------------+\n",
      "|  a|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|\n",
      "|  b|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|\n",
      "+---+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ElementwiseProduct\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "// Create some vector data; also works for sparse vectors\n",
    "val dataFrame = sqlContext.createDataFrame(Seq(\n",
    "  (\"a\", Vectors.dense(1.0, 2.0, 3.0)),\n",
    "  (\"b\", Vectors.dense(4.0, 5.0, 6.0)))).toDF(\"id\", \"vector\")\n",
    "\n",
    "val transformingVector = Vectors.dense(0.0, 1.0, 2.0)\n",
    "val transformer = new ElementwiseProduct().setScalingVec(transformingVector).setInputCol(\"vector\").setOutputCol(\"transformedVector\")\n",
    "\n",
    "// Batch transform the vectors to create new column:\n",
    "transformer.transform(dataFrame).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.SQLTransformer\n",
    "\n",
    "val df = sqlContext.createDataFrame(\n",
    "  Seq((0, 1.0, 3.0), (2, 2.0, 5.0))).toDF(\"id\", \"v1\", \"v2\")\n",
    "\n",
    "val sqlTrans = new SQLTransformer().setStatement(\n",
    "  \"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "\n",
    "sqlTrans.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+--------------+-------+\n",
      "| id|hour|mobile|  userFeatures|clicked|\n",
      "+---+----+------+--------------+-------+\n",
      "|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n",
      "+---+----+------+--------------+-------+\n",
      "\n",
      "[[18.0,1.0,0.0,10.0,0.5],1.0]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val dataset = sqlContext.createDataFrame(\n",
    "  Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))\n",
    ").toDF(\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\")\n",
    "dataset.show()\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"hour\", \"mobile\", \"userFeatures\")).setOutputCol(\"features\")\n",
    "\n",
    "val output = assembler.transform(dataset)\n",
    "println(output.select(\"features\", \"clicked\").first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuantileDiscretizer (HATA VAR !!)\n",
    "http://stackoverflow.com/questions/32522942/why-does-spark-scala-compiler-fail-to-find-todf-on-rddmapint-int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:87: error: value toDF is not a member of org.apache.spark.rdd.RDD[(Int, Double)]\n",
       "         val df = sc.parallelize(data).toDF(\"id\", \"hour\")\n",
       "                                       ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "\n",
    "val data = Array((0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2))\n",
    "val df = sc.parallelize(data).toDF(\"id\", \"hour\")\n",
    "\n",
    "val discretizer = new QuantileDiscretizer().setInputCol(\"hour\").setOutputCol(\"result\").setNumBuckets(3)\n",
    "\n",
    "val result = discretizer.fit(df).transform(df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0,2.3,0.0],[2.3,0.0]]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.attribute.{Attribute, AttributeGroup, NumericAttribute}\n",
    "import org.apache.spark.ml.feature.VectorSlicer\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.StructType\n",
    "\n",
    "val data = Array(Row(Vectors.dense(-2.0, 2.3, 0.0)))\n",
    "\n",
    "val defaultAttr = NumericAttribute.defaultAttr\n",
    "val attrs = Array(\"f1\", \"f2\", \"f3\").map(defaultAttr.withName)\n",
    "val attrGroup = new AttributeGroup(\"userFeatures\", attrs.asInstanceOf[Array[Attribute]])\n",
    "\n",
    "val dataRDD = sc.parallelize(data)\n",
    "val dataset = sqlContext.createDataFrame(dataRDD, StructType(Array(attrGroup.toStructField())))\n",
    "\n",
    "val slicer = new VectorSlicer().setInputCol(\"userFeatures\").setOutputCol(\"features\")\n",
    "\n",
    "slicer.setIndices(Array(1)).setNames(Array(\"f3\"))\n",
    "// or slicer.setIndices(Array(1, 2)), or slicer.setNames(Array(\"f2\", \"f3\"))\n",
    "\n",
    "val output = slicer.transform(dataset)\n",
    "println(output.select(\"userFeatures\", \"features\").first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[0.0,0.0,18.0]|  1.0|\n",
      "|[0.0,1.0,12.0]|  0.0|\n",
      "|[1.0,0.0,15.0]|  0.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RFormula\n",
    "\n",
    "val dataset = sqlContext.createDataFrame(Seq(\n",
    "  (7, \"US\", 18, 1.0),\n",
    "  (8, \"CA\", 12, 0.0),\n",
    "  (9, \"NZ\", 15, 0.0)\n",
    ")).toDF(\"id\", \"country\", \"hour\", \"clicked\")\n",
    "val formula = new RFormula().setFormula(\"clicked ~ country + hour\").setFeaturesCol(\"features\").setLabelCol(\"label\")\n",
    "val output = formula.fit(dataset).transform(dataset)\n",
    "output.select(\"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChiSqSelector (HATA VAR!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:97: error: value toDF is not a member of org.apache.spark.rdd.RDD[(Int, org.apache.spark.mllib.linalg.Vector, Double)]\n",
       "       val df = sc.parallelize(data).toDF(\"id\", \"features\", \"clicked\")\n",
       "                                     ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val data = Seq(\n",
    "  (7, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0),\n",
    "  (8, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0),\n",
    "  (9, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0)\n",
    ")\n",
    "\n",
    "val df = sc.parallelize(data).toDF(\"id\", \"features\", \"clicked\")\n",
    "\n",
    "val selector = new ChiSqSelector().setNumTopFeatures(1).setFeaturesCol(\"features\").setLabelCol(\"clicked\").setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "val result = selector.fit(df).transform(df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.0 (Scala 2.10.6)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
